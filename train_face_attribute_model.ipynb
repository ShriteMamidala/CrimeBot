{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SELECTED_ATTRIBUTES = ['Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby',\n",
    "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
    "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
    "       'Rosy_Cheeks', 'Sideburns', 'Straight_Hair', 'Wavy_Hair',\n",
    "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
    "       'Wearing_Necklace', 'Wearing_Necktie']\n",
    "IMG_DIR = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\Ml_Projects\\Data\\celeba\\img_align_celeba\"\n",
    "ATTR_FILE = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\Ml_Projects\\Data\\celeba\\list_attr_celeba.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, attr_file, selected_attributes, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # load/preprocess attributes\n",
    "        self.attributes = pd.read_csv(attr_file, delim_whitespace=True, skiprows=1)\n",
    "\n",
    "        # fix filenames\n",
    "        self.attributes.index = self.attributes.index.map(lambda x: f\"{x.zfill(6)}.jpg\" if not x.endswith(\".jpg\") else x)\n",
    "        \n",
    "        # make binary\n",
    "        self.attributes = self.attributes[selected_attributes].replace(-1, 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attributes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image name and path\n",
    "        img_name = self.attributes.index[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # open image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: {e} - Image {img_name} not found.\")\n",
    "            raise e\n",
    "\n",
    "        # Get labels as a tensor\n",
    "        labels = torch.tensor(self.attributes.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        # apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop((160, 160)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "#augmentatiuons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrit\\AppData\\Local\\Temp\\ipykernel_16688\\3189453638.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  self.attributes = pd.read_csv(attr_file, delim_whitespace=True, skiprows=1)\n"
     ]
    }
   ],
   "source": [
    "dataset = CelebADataset(img_dir=IMG_DIR, attr_file=ATTR_FILE, selected_attributes=SELECTED_ATTRIBUTES, transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.dataset.transform = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAttributeModel(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(FaceAttributeModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1) # imagenet weights, mobilenetV3\n",
    "        in_features = self.base_model.classifier[0].in_features\n",
    "        self.base_model.classifier = nn.Sequential()  #  original classifier\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 512)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "#actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0  #  accuracy counters\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Accuracy \n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "    accuracy = correct / total  \n",
    "    print(f\"Training Accuracy: {accuracy:.4f}\")  #  accuracy\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0  #  accuracy counters\n",
    "\n",
    "    with torch.no_grad(): # no back prop\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    accuracy = correct / total \n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")  # accuracy\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceAttributeModel(num_features=len(SELECTED_ATTRIBUTES)).to(DEVICE)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")  # GPU\n",
    "print(next(model.parameters()).device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9138\n",
      "Best model state_dict saved with validation loss: 0.1966\n",
      "Train Loss: 0.2383, Val Loss: 0.1966\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9176\n",
      "Best model state_dict saved with validation loss: 0.1873\n",
      "Train Loss: 0.1957, Val Loss: 0.1873\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9192\n",
      "Best model state_dict saved with validation loss: 0.1838\n",
      "Train Loss: 0.1876, Val Loss: 0.1838\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9194\n",
      "Best model state_dict saved with validation loss: 0.1829\n",
      "Train Loss: 0.1821, Val Loss: 0.1829\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9199\n",
      "Best model state_dict saved with validation loss: 0.1823\n",
      "Train Loss: 0.1774, Val Loss: 0.1823\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, DEVICE)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Accuracy calculation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir = \"C:\\\\Users\\\\shrit\\\\Desktop\\\\Ml_Projects\\\\Ml_Projects\\\\pytorch_results_weights\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_model_path = os.path.join(save_dir, \"best_model_state_dict.pth\")  #  path for state_dict\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "\n",
    "    # Training and validation\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save  state_dict\n",
    "        print(f\"Best model state_dict saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9198\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "             Bald       0.80      0.75      0.77       930\n",
      "            Bangs       0.88      0.85      0.86      6140\n",
      "         Big_Lips       0.60      0.37      0.46      9834\n",
      "         Big_Nose       0.72      0.59      0.65      9421\n",
      "       Black_Hair       0.81      0.80      0.81      9740\n",
      "       Blond_Hair       0.86      0.86      0.86      6008\n",
      "       Brown_Hair       0.74      0.65      0.69      8354\n",
      "   Bushy_Eyebrows       0.80      0.59      0.68      5678\n",
      "           Chubby       0.68      0.52      0.59      2306\n",
      "       Eyeglasses       0.98      0.97      0.97      2681\n",
      "           Goatee       0.78      0.76      0.77      2518\n",
      "        Gray_Hair       0.81      0.72      0.76      1750\n",
      "     Heavy_Makeup       0.90      0.90      0.90     15781\n",
      "  High_Cheekbones       0.89      0.85      0.87     18555\n",
      "         Mustache       0.66      0.54      0.59      1668\n",
      "      Narrow_Eyes       0.63      0.40      0.49      4660\n",
      "         No_Beard       0.97      0.98      0.98     33942\n",
      "        Oval_Face       0.65      0.44      0.52     11622\n",
      "        Pale_Skin       0.61      0.62      0.61      1773\n",
      "      Pointy_Nose       0.65      0.45      0.53     11185\n",
      "Receding_Hairline       0.71      0.54      0.61      3241\n",
      "      Rosy_Cheeks       0.72      0.48      0.58      2665\n",
      "        Sideburns       0.79      0.79      0.79      2235\n",
      "    Straight_Hair       0.69      0.54      0.60      8245\n",
      "        Wavy_Hair       0.78      0.78      0.78     12949\n",
      " Wearing_Earrings       0.79      0.76      0.77      7806\n",
      "      Wearing_Hat       0.92      0.90      0.91      1970\n",
      " Wearing_Lipstick       0.93      0.94      0.94     19215\n",
      " Wearing_Necklace       0.56      0.15      0.23      5011\n",
      "  Wearing_Necktie       0.69      0.73      0.71      2947\n",
      "\n",
      "        micro avg       0.83      0.73      0.78    230830\n",
      "        macro avg       0.77      0.67      0.71    230830\n",
      "     weighted avg       0.81      0.73      0.76    230830\n",
      "      samples avg       0.82      0.72      0.74    230830\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\shrit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\shrit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predictions = (outputs > 0.5).float() \n",
    "\n",
    "        all_preds.append(predictions.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "accuracy = accuracy_score(all_labels.flatten(), all_preds.flatten())\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=SELECTED_ATTRIBUTES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector (Probabilities):\n",
      "[0.50000036 0.5000213  0.54518944 0.5009237  0.5004308  0.5054338\n",
      " 0.50880396 0.50006986 0.5000099  0.50004214 0.5000024  0.5000016\n",
      " 0.5109977  0.52519935 0.5000051  0.5157874  0.730968   0.50299853\n",
      " 0.5021717  0.53430116 0.5002037  0.50001615 0.500003   0.7090483\n",
      " 0.5002853  0.5486195  0.534381   0.57198733 0.51722044 0.50000536]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "model.eval() \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "image_path = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\Ml_Projects\\Data\\celeba\\img_align_celeba\\000001.jpg\"  \n",
    "raw_image = Image.open(image_path).convert('RGB')  \n",
    "image = preprocess(raw_image) \n",
    "\n",
    "image = image.unsqueeze(0).to(DEVICE)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)  \n",
    "\n",
    "probabilities = torch.sigmoid(outputs).cpu().numpy().flatten()  \n",
    "\n",
    "feature_vector = probabilities\n",
    "\n",
    "print(\"Feature Vector (Probabilities):\")\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
